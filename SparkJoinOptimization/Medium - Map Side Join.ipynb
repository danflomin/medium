{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "from pyspark.sql.functions import udf, col, broadcast\n",
    "import random\n",
    "\n",
    "spark.sparkContext.setCheckpointDir(\"./spark_tmp/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_context = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dictionary Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = []\n",
    "for context in range(num_context):\n",
    "    for word_id in range(20000):\n",
    "        word = str(hash(random.random()))\n",
    "        data.append((context, word_id, word))\n",
    "df = spark.createDataFrame(data, ['context', 'word_id', 'word'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"dictionary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Coded Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = [(6, (i*3+5)%566, (i*60+6)%13) for i in range(10000)] # some random data\n",
    "data = [(6, (i*3+5)%5000, (i*60+6)%13) for i in range(100000)] # some random data\n",
    "coded_df = spark.createDataFrame(data, ['context', 'word_id', 'word_id2']).distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "coded_df.createOrReplaceTempView(\"coded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let the joining begin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1st try - SQL Syntax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 24 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sql_df = spark.sql(\"\"\"\n",
    "select a.context, a.word_id, b.word, a.word_id2, c.word as word2\n",
    "from coded a\n",
    "join dictionary b\n",
    "on a.context = b.context and a.word_id = b.word_id \n",
    "join dictionary c\n",
    "on a.context = c.context and a.word_id2 = c.word_id\n",
    "\"\"\")\n",
    "sql_count = sql_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65000"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2nd try - Spark SQL Syntax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7358"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coded_df.join(df, (coded_df['word_id'] == df['word_id']) & (coded_df['context'] == df['context'])).select(coded_df['word_id2'], df['*']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "spark_sql_df = coded_df.join(df, (coded_df['word_id'] == df['word_id']) & (coded_df['context'] == df['context'])).select(coded_df['word_id2'], df['*'])\n",
    "spark_sql_df = spark_sql_df.checkpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) Scan ExistingRDD[word_id2#8L,context#0L,word_id#1L,word#2]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_sql_df.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+-------+-------------------+-------+-------+-------------------+\n",
      "|word_id2|context|word_id|               word|context|word_id|               word|\n",
      "+--------+-------+-------+-------------------+-------+-------+-------------------+\n",
      "|       1|      6|      1|1968718652097126144|      6|      0|1166852827755916544|\n",
      "|       1|      6|      1|1968718652097126144|      6|      1|1968718652097126144|\n",
      "|       1|      6|      1|1968718652097126144|      6|      2|1714623463938671872|\n",
      "|       1|      6|      1|1968718652097126144|      6|      3|2140051419228967168|\n",
      "|       1|      6|      1|1968718652097126144|      6|      4| 417743325427254784|\n",
      "|       1|      6|      1|1968718652097126144|      6|      5| 225461857228028928|\n",
      "|       1|      6|      1|1968718652097126144|      6|      6|1855172547184932608|\n",
      "|       1|      6|      1|1968718652097126144|      6|      7| 150798719757318400|\n",
      "|       1|      6|      1|1968718652097126144|      6|      8| 865651495621323520|\n",
      "|       1|      6|      1|1968718652097126144|      6|      9|2231007765421255680|\n",
      "|       1|      6|      1|1968718652097126144|      6|     10|1974420278180599040|\n",
      "|       1|      6|      1|1968718652097126144|      6|     11|2273056638107935744|\n",
      "|       1|      6|      1|1968718652097126144|      6|     12| 199352014537699840|\n",
      "|       1|      6|      1|1968718652097126144|      6|     13| 948750099048717056|\n",
      "|       1|      6|      1|1968718652097126144|      6|     14| 662602637995565824|\n",
      "|       1|      6|      1|1968718652097126144|      6|     15| 320026612638588416|\n",
      "|       1|      6|      1|1968718652097126144|      6|     16|1765536811481554432|\n",
      "|       1|      6|      1|1968718652097126144|      6|     17|1559557767871953664|\n",
      "|       1|      6|      1|1968718652097126144|      6|     18|1168606453770786304|\n",
      "|       1|      6|      1|1968718652097126144|      6|     19| 568974510464646912|\n",
      "+--------+-------+-------+-------------------+-------+-------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# .select(spark_sql_df['*'], df['word'].alias('word2'))\n",
    "spark_sql_df.where('word_id2 = 1').join(df, ((spark_sql_df['word_id2'] == df['word_id']) & (spark_sql_df['context'] == df['context']))).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65000"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_sql_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3rd try - the plain old dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the dictionary data structure is indexed, therefore fetches data fast\n",
    "indexed_data = {ctx:dict() for ctx in range(num_context)}\n",
    "for context, word_id, word in data:\n",
    "    indexed_data[context][word_id] = word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "broadcast_indexed_data = spark.sparkContext.broadcast(indexed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "join_udf = udf(lambda ctx, word_id: broadcast_indexed_data.value[ctx][word_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "udf_df = coded_df.withColumn('word', join_udf(col(\"context\"), col(\"word_id\")))\n",
    "udf_df = udf_df.withColumn('word2', join_udf(col(\"context\"), col(\"word_id2\")))\n",
    "udf_count = udf_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65000"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "udf_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Digging deeper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(10) Project [context#2116L, word_id#2117L, word#1954, word_id2#2118L, word#2128 AS word2#2125]\n",
      "+- *(10) SortMergeJoin [context#2116L, word_id2#2118L], [context#2126L, word_id#2127L], Inner\n",
      "   :- *(7) Sort [context#2116L ASC NULLS FIRST, word_id2#2118L ASC NULLS FIRST], false, 0\n",
      "   :  +- Exchange hashpartitioning(context#2116L, word_id2#2118L, 200), true, [id=#4493]\n",
      "   :     +- *(6) Project [context#2116L, word_id#2117L, word_id2#2118L, word#1954]\n",
      "   :        +- *(6) SortMergeJoin [context#2116L, word_id#2117L], [context#1952L, word_id#1953L], Inner\n",
      "   :           :- *(3) Sort [context#2116L ASC NULLS FIRST, word_id#2117L ASC NULLS FIRST], false, 0\n",
      "   :           :  +- Exchange hashpartitioning(context#2116L, word_id#2117L, 200), true, [id=#4479]\n",
      "   :           :     +- *(2) HashAggregate(keys=[context#2116L, word_id#2117L, word_id2#2118L], functions=[])\n",
      "   :           :        +- Exchange hashpartitioning(context#2116L, word_id#2117L, word_id2#2118L, 200), true, [id=#4475]\n",
      "   :           :           +- *(1) HashAggregate(keys=[context#2116L, word_id#2117L, word_id2#2118L], functions=[])\n",
      "   :           :              +- *(1) Filter ((isnotnull(context#2116L) AND isnotnull(word_id#2117L)) AND isnotnull(word_id2#2118L))\n",
      "   :           :                 +- *(1) Scan ExistingRDD[context#2116L,word_id#2117L,word_id2#2118L]\n",
      "   :           +- *(5) Sort [context#1952L ASC NULLS FIRST, word_id#1953L ASC NULLS FIRST], false, 0\n",
      "   :              +- Exchange hashpartitioning(context#1952L, word_id#1953L, 200), true, [id=#4485]\n",
      "   :                 +- *(4) Filter (isnotnull(context#1952L) AND isnotnull(word_id#1953L))\n",
      "   :                    +- *(4) Scan ExistingRDD[context#1952L,word_id#1953L,word#1954]\n",
      "   +- *(9) Sort [context#2126L ASC NULLS FIRST, word_id#2127L ASC NULLS FIRST], false, 0\n",
      "      +- ReusedExchange [context#2126L, word_id#2127L, word#2128], Exchange hashpartitioning(context#1952L, word_id#1953L, 200), true, [id=#4485]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sql_df.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark_sql_df.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(3) Project [context#2116L, word_id#2117L, word_id2#2118L, pythonUDF0#2175 AS word#2153, pythonUDF1#2176 AS word2#2159]\n",
      "+- BatchEvalPython [<lambda>(context#2116L, word_id#2117L), <lambda>(context#2116L, word_id2#2118L)], [pythonUDF0#2175, pythonUDF1#2176]\n",
      "   +- *(2) HashAggregate(keys=[context#2116L, word_id#2117L, word_id2#2118L], functions=[])\n",
      "      +- Exchange hashpartitioning(context#2116L, word_id#2117L, word_id2#2118L, 200), true, [id=#4564]\n",
      "         +- *(1) HashAggregate(keys=[context#2116L, word_id#2117L, word_id2#2118L], functions=[])\n",
      "            +- *(1) Scan ExistingRDD[context#2116L,word_id#2117L,word_id2#2118L]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "udf_df.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# how does it change as the amount of colunms to translate?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
