{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 38.033935546875,
      "end_time": 1603739905147.744
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>4</td><td>application_1603734684199_0010</td><td>pyspark3</td><td>idle</td><td><a target=\"_blank\" href=\"http://hn1-roastb.hocshyj43bzedeejcqcxxqde1b.bx.internal.cloudapp.net:8088/proxy/application_1603734684199_0010/\">Link</a></td><td><a target=\"_blank\" href=\"http://wn3-roastb.hocshyj43bzedeejcqcxxqde1b.bx.internal.cloudapp.net:30060/node/containerlogs/container_1603734684199_0010_01_000001/livy\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "# Python 3.5.2\n",
    "# PySpark 2.4.5.4.1.1.2\n",
    "# Azure's HDInsight 4.0 (2 master nodes, 8 worker nodes - 16 cores and 112 GB ram each)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** pyspark imports ** \n",
    "1. udf is for the withColumn join \n",
    "2. broadcast is for broadcasting mapping_df before joining in sql_df \n",
    "3. the default is without broadcast - check get_sql_df function\n",
    "\n",
    "** random, itertools imports ** \n",
    "\n",
    "For the generation of mapping_df's columns.\n",
    "random used in the generation of product_name column \n",
    "itertools used in the generation of (department_id, product_id) columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 239.679931640625,
      "end_time": 1603739905396.588
     }
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf, col, broadcast\n",
    "import random\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 33.702880859375,
      "end_time": 1603739905438.559
     }
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_departments = 15\n",
    "num_product_id = 20000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 2 following functions are used in our experiments.\n",
    "\n",
    "We check how much time running the action takes using **time_function**.\n",
    "We calculate an average of several runs using **avg_time_function**.\n",
    "\n",
    "Arguments: **func** is the function to run, and **args** are its arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 37.610107421875,
      "end_time": 1603739905484.286
     }
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def time_function(func, *args):\n",
    "    import time \n",
    "    t1 = time.time()\n",
    "    result = func(*args)\n",
    "    t2 = time.time()\n",
    "    total_time = t2-t1\n",
    "    return total_time, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 35.9638671875,
      "end_time": 1603739905529.079
     }
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def avg_time_function(iters, func, *args):\n",
    "    total_time = 0\n",
    "    for _ in range(iters):\n",
    "        time, _ = time_function(func, *args)\n",
    "        total_time += time\n",
    "    return (total_time/iters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dictionary Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we create the dataframe that holds the names for different product_id's of the different departments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 747.011962890625,
      "end_time": 1603739906285.934
     }
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "base_data = itertools.product(range(num_departments), range(num_product_id))\n",
    "mapped_data = map(lambda tup: (tup[0], tup[1], str(hash(random.random())%6789)), base_data)\n",
    "mapping_data = list(mapped_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 19595.779052734375,
      "end_time": 1603739925890.198
     }
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mapping_df = spark.createDataFrame(mapping_data, ['department_id', 'product_id', 'product_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 5285.469970703125,
      "end_time": 1603739931184.395
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300000"
     ]
    }
   ],
   "source": [
    "mapping_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Coded Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function creates a dataframe, with **num_rows** rows, and **num_cols** columns of **product_id**.\n",
    "\n",
    "Later on we will join each such column to get the name of each such **product_id**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 27.5380859375,
      "end_time": 1603739931291.953
     }
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_data_df(num_rows, num_cols):\n",
    "    did = \"id % {} as department_id\".format(num_departments)\n",
    "    data_df = spark.range(num_rows).selectExpr(did)\n",
    "    for i in range(num_cols):\n",
    "        pid = \"floor(rand()*{})\".format(num_product_id)\n",
    "        data_df = data_df.selectExpr(\"*\", \"floor(rand()*{}) as product_id{}\".format(num_product_id,i))\n",
    "    return data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 739.7041015625,
      "end_time": 1603739932039.777
     }
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_rows = 10000\n",
    "data_df = get_data_df(num_rows, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 788.947021484375,
      "end_time": 1603739932837.472
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------+-----------+-----------+-----------+-----------+\n",
      "|department_id|product_id0|product_id1|product_id2|product_id3|product_id4|\n",
      "+-------------+-----------+-----------+-----------+-----------+-----------+\n",
      "|            0|       7000|       2544|       7239|      17716|      16790|\n",
      "|            1|      17938|       4821|      14477|        950|      16758|\n",
      "|            2|       9842|      14424|      12156|      14403|      16541|\n",
      "|            3|      17608|       8616|       5722|       8825|      15340|\n",
      "|            4|       7495|       7624|      17885|      17330|       2680|\n",
      "|            5|       2608|       9340|      15481|      16591|      18968|\n",
      "|            6|      11770|      12593|      10014|      12227|       9925|\n",
      "|            7|      11765|      16057|       1201|        901|       2220|\n",
      "|            8|      13440|      18077|        400|      12440|      15988|\n",
      "|            9|      11389|       5505|      17030|      17371|      15744|\n",
      "|           10|      15471|      10016|      16706|      18050|      11674|\n",
      "|           11|      17123|      18758|      16018|      19063|       5269|\n",
      "|           12|        281|       2264|      16209|      15316|      11882|\n",
      "|           13|       7295|       1764|      10205|      10118|      19688|\n",
      "|           14|       1563|       6710|      11343|      16437|      17778|\n",
      "|            0|       6675|      17496|       6963|       7070|       8001|\n",
      "|            1|      13943|       3567|      16014|      14715|      18245|\n",
      "|            2|       1949|      18499|         96|      17505|       3370|\n",
      "|            3|      16008|       9664|       8025|       7290|       3357|\n",
      "|            4|       3030|      14965|       9774|       9549|      12233|\n",
      "|            5|      17274|      12135|      12846|       6347|       7131|\n",
      "|            6|      13383|       7622|      11742|      11665|      13603|\n",
      "|            7|       6621|      14721|      19787|       9158|        239|\n",
      "|            8|       2878|      13009|      16559|       9240|      19256|\n",
      "|            9|      12871|       4871|       6904|       6326|      15498|\n",
      "|           10|      18902|      17670|       5554|       4001|      10510|\n",
      "|           11|      16599|       9948|       9431|      12386|       7342|\n",
      "|           12|      10257|       3211|      12291|      10232|       6644|\n",
      "|           13|      14364|       5723|      13453|      18578|       2165|\n",
      "|           14|      14398|       2179|      16798|       3864|       9977|\n",
      "|            0|      15082|       2692|       3099|       6111|      12201|\n",
      "|            1|       1219|       7856|       1057|      14302|       3287|\n",
      "|            2|      13821|      12169|      11790|       8312|       1029|\n",
      "|            3|      11983|      18956|       9306|       2254|      14640|\n",
      "|            4|      15169|       6949|      13232|      14828|       6129|\n",
      "+-------------+-----------+-----------+-----------+-----------+-----------+\n",
      "only showing top 35 rows"
     ]
    }
   ],
   "source": [
    "data_df.show(35)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let the joining begin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1st try - SQL Syntax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function does the following:\n",
    "\n",
    "1. For each **product_id** column, we join to mapping_df to get its **product_name**.\n",
    "2. After finished joining all **product_id** columns, we group by all **product_name** columns and *count()* it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 86.006103515625,
      "end_time": 1603739932932.363
     }
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_sql_df(data_df, mapping_df, num_cols):\n",
    "#     mapping_df = broadcast(mapping_df)\n",
    "    data_df.createOrReplaceTempView(\"data\")\n",
    "    mapping_df.createOrReplaceTempView(\"mapping\")\n",
    "    def query_generator(num_cols):\n",
    "        select_clause = \" , \".join([\"b{}.product_name as product_name{}\".format(i,i) for i in range(num_cols)])\n",
    "        join_clause = \" \\n \".join([\"join mapping b{} on a.department_id = b{}.department_id and a.product_id{} = b{}.product_id\".format(i,i,i,i) for i in range(num_cols)])\n",
    "        sql = \"\"\"\n",
    "                select a.*, {} \n",
    "                from data a\n",
    "                {}\n",
    "                \"\"\".format(select_clause, join_clause)\n",
    "        return sql\n",
    "        \n",
    "    sql_df = spark.sql(query_generator(num_cols))\n",
    "    product_name_columns_names = [\"product_name{}\".format(i) for i in range(num_cols)]\n",
    "    sql_df = sql_df.groupBy(product_name_columns_names).count()\n",
    "    return sql_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 7629.677978515625,
      "end_time": 1603739940573.472
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[product_name0: string, product_name1: string, product_name2: string, product_name3: string, product_name4: string, count: bigint]"
     ]
    }
   ],
   "source": [
    "get_sql_df(data_df, mapping_df, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2nd try - the plain old dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we do the following:\n",
    "\n",
    "1. Collect **mapping_df**.\n",
    "2. Create a dictionary with 2 hierarchies, **department_id** as its first hierarchy. The second hierarchy is **product_id** with **product_name** as its value.\n",
    "3. Broadcast the dictionary to all executors.\n",
    "4. Create a udf that maps a couple (**deparment_id**, **product_id**) into **product_name** using the broadcasted dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 3377.6259765625,
      "end_time": 1603739943959.826
     }
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "indexed_data = dict()\n",
    "for department_id, product_id, product_name in mapping_df.collect():\n",
    "    if department_id not in indexed_data:\n",
    "        indexed_data[department_id] = dict()\n",
    "    indexed_data[department_id][product_id] = product_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 236.674072265625,
      "end_time": 1603739944204.933
     }
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "broadcast_indexed_data = spark.sparkContext.broadcast(indexed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 32.260986328125,
      "end_time": 1603739944245.024
     }
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "join_udf = udf(lambda department_id, product_id: broadcast_indexed_data.value[department_id][product_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is similar to get_sql_df. The different is that instead of joining as in get_sql_df's first phase, we map each **product_id** column along with the **depratment_id** on its row into **product_name**.\n",
    "\n",
    "The functionality is the same, the technicality is different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 31.8759765625,
      "end_time": 1603739949254.273
     }
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_udf_df(data_df, num_cols):\n",
    "    udf_df = data_df\n",
    "    for i in range(num_cols):\n",
    "        udf_df = udf_df.withColumn('product_name{}'.format(i), join_udf(col(\"department_id\"), col(\"product_id{}\".format(i))))\n",
    "    product_name_columns_names = [\"product_name{}\".format(i) for i in range(num_cols)]\n",
    "    udf_df = udf_df.groupBy(*product_name_columns_names).count()\n",
    "    return udf_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Who is faster?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below is used in **avg_time_function** to check the performance of the different join approaches.\n",
    "\n",
    "**process_df** is **avg_time_function**'s *func* parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 30.84814453125,
      "end_time": 1603739951969.507
     }
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_df(df):\n",
    "    df_count = df.count()\n",
    "    return df, df_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 33.157958984375,
      "end_time": 1603739963148.392
     }
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 100 thousand in a variable for simplicity\n",
    "k100=100000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**print_avg** is our experimenting function.\n",
    "\n",
    "It creates *data_df* with **num_rows** rows and **num_cols** columns.\n",
    "Then it joins *data_df* in the 2 different approaches using *get_sql_df* and *get_sql_df*.\n",
    "\n",
    "After creating the dataframes we will process using *process_df* function, we use **avg_time_function** to get the relevant running time averages for each of 2 dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 29.546875,
      "end_time": 1603739966321.954
     }
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_avg(num_iterations, num_rows, num_cols=10):\n",
    "    data_df = get_data_df(num_rows, num_cols)\n",
    "    \n",
    "    sql_df = get_sql_df(data_df, mapping_df, num_cols)\n",
    "    udf_df = get_udf_df(data_df, num_cols)\n",
    "    \n",
    "    for df, name in [(sql_df, 'sql_df'), (udf_df, 'udf_df')]:\n",
    "        average_run_time = avg_time_function(num_iterations, process_df, df)\n",
    "        print(\"average run_time for {name} with {num_rows} rows is {average_run_time}\".format(name=name, average_run_time=average_run_time, num_rows=num_rows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 58516.97802734375,
      "end_time": 1603740027016.98
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average run_time for sql_df with 100000 rows is 9.40359115600586\n",
      "average run_time for udf_df with 100000 rows is 1.8074284076690674"
     ]
    }
   ],
   "source": [
    "print_avg(num_iterations=5, num_rows=k100, num_cols=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 93041.3310546875,
      "end_time": 1603740260914.206
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average run_time for sql_df with 100000 rows is 16.00410599708557\n",
      "average run_time for udf_df with 100000 rows is 2.181958818435669"
     ]
    }
   ],
   "source": [
    "print_avg(num_iterations=5, num_rows=k100, num_cols=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 155874.39599609375,
      "end_time": 1603740436601.063
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average run_time for sql_df with 100000 rows is 27.68385329246521\n",
      "average run_time for udf_df with 100000 rows is 3.11617431640625"
     ]
    }
   ],
   "source": [
    "print_avg(num_iterations=5, num_rows=k100, num_cols=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 106058.70678710938,
      "end_time": 1603740584220.798
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average run_time for sql_df with 1000000 rows is 17.9197660446167\n",
      "average run_time for udf_df with 1000000 rows is 2.8207101821899414"
     ]
    }
   ],
   "source": [
    "print_avg(num_iterations=5, num_rows=10*k100, num_cols=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 116423.7431640625,
      "end_time": 1603740716764.995
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average run_time for sql_df with 3000000 rows is 19.310581111907958\n",
      "average run_time for udf_df with 3000000 rows is 3.5375089168548586"
     ]
    }
   ],
   "source": [
    "print_avg(num_iterations=5, num_rows=30*k100, num_cols=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 153223.19995117188,
      "end_time": 1603740877365.288
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average run_time for sql_df with 10000000 rows is 23.55312309265137\n",
      "average run_time for udf_df with 10000000 rows is 6.80124945640564"
     ]
    }
   ],
   "source": [
    "print_avg(num_iterations=5, num_rows=100*k100, num_cols=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 352939.5300292969,
      "end_time": 1603741243393.468
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average run_time for sql_df with 50000000 rows is 45.740611982345584\n",
      "average run_time for udf_df with 50000000 rows is 24.278624248504638"
     ]
    }
   ],
   "source": [
    "print_avg(num_iterations=5, num_rows=500*k100, num_cols=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the plan?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running *.explain()* below helps understanding Spark's under the hood regarding the 2 different approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 1504.668212890625,
      "end_time": 1603741553974.649
     }
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_cols = 5\n",
    "data_df = get_data_df(k100, num_cols)\n",
    "sql_df = get_sql_df(data_df, mapping_df, num_cols)\n",
    "udf_df = get_udf_df(data_df, num_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 889.85400390625,
      "end_time": 1603741557674.698
     }
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(19) HashAggregate(keys=[product_name0#5632, product_name1#5633, product_name2#5634, product_name3#5635, product_name4#5636], functions=[count(1)])\n",
      "+- Exchange hashpartitioning(product_name0#5632, product_name1#5633, product_name2#5634, product_name3#5635, product_name4#5636, 200)\n",
      "   +- *(18) HashAggregate(keys=[product_name0#5632, product_name1#5633, product_name2#5634, product_name3#5635, product_name4#5636], functions=[partial_count(1)])\n",
      "      +- *(18) Project [product_name#2 AS product_name0#5632, product_name#5639 AS product_name1#5633, product_name#5642 AS product_name2#5634, product_name#5645 AS product_name3#5635, product_name#5648 AS product_name4#5636]\n",
      "         +- *(18) SortMergeJoin [department_id#5605L, product_id4#5625L], [department_id#5646L, product_id#5647L], Inner\n",
      "            :- *(15) Sort [department_id#5605L ASC NULLS FIRST, product_id4#5625L ASC NULLS FIRST], false, 0\n",
      "            :  +- Exchange hashpartitioning(department_id#5605L, product_id4#5625L, 200)\n",
      "            :     +- *(14) Project [department_id#5605L, product_id4#5625L, product_name#2, product_name#5639, product_name#5642, product_name#5645]\n",
      "            :        +- *(14) SortMergeJoin [department_id#5605L, product_id3#5619L], [department_id#5643L, product_id#5644L], Inner\n",
      "            :           :- *(11) Sort [department_id#5605L ASC NULLS FIRST, product_id3#5619L ASC NULLS FIRST], false, 0\n",
      "            :           :  +- Exchange hashpartitioning(department_id#5605L, product_id3#5619L, 200)\n",
      "            :           :     +- *(10) Project [department_id#5605L, product_id3#5619L, product_id4#5625L, product_name#2, product_name#5639, product_name#5642]\n",
      "            :           :        +- *(10) SortMergeJoin [department_id#5605L, product_id2#5614L], [department_id#5640L, product_id#5641L], Inner\n",
      "            :           :           :- *(7) Sort [department_id#5605L ASC NULLS FIRST, product_id2#5614L ASC NULLS FIRST], false, 0\n",
      "            :           :           :  +- Exchange hashpartitioning(department_id#5605L, product_id2#5614L, 200)\n",
      "            :           :           :     +- *(6) Project [department_id#5605L, product_id2#5614L, product_id3#5619L, product_id4#5625L, product_name#2, product_name#5639]\n",
      "            :           :           :        +- *(6) SortMergeJoin [department_id#5605L, product_id1#5610L], [department_id#5637L, product_id#5638L], Inner\n",
      "            :           :           :           :- *(3) Sort [department_id#5605L ASC NULLS FIRST, product_id1#5610L ASC NULLS FIRST], false, 0\n",
      "            :           :           :           :  +- Exchange hashpartitioning(department_id#5605L, product_id1#5610L, 200)\n",
      "            :           :           :           :     +- *(2) Project [department_id#5605L, product_id1#5610L, product_id2#5614L, product_id3#5619L, product_id4#5625L, product_name#2]\n",
      "            :           :           :           :        +- *(2) BroadcastHashJoin [department_id#5605L, product_id0#5607L], [department_id#0L, product_id#1L], Inner, BuildLeft\n",
      "            :           :           :           :           :- BroadcastExchange HashedRelationBroadcastMode(List(input[0, bigint, false], input[1, bigint, false]))\n",
      "            :           :           :           :           :  +- *(1) Filter (((((isnotnull(department_id#5605L) && isnotnull(product_id0#5607L)) && isnotnull(product_id1#5610L)) && isnotnull(product_id2#5614L)) && isnotnull(product_id3#5619L)) && isnotnull(product_id4#5625L))\n",
      "            :           :           :           :           :     +- *(1) Project [department_id#5605L, product_id0#5607L, product_id1#5610L, product_id2#5614L, product_id3#5619L, FLOOR((rand(-1393926809299574983) * 20000.0)) AS product_id4#5625L]\n",
      "            :           :           :           :           :        +- *(1) Project [department_id#5605L, product_id0#5607L, product_id1#5610L, product_id2#5614L, FLOOR((rand(8479318887747898147) * 20000.0)) AS product_id3#5619L]\n",
      "            :           :           :           :           :           +- *(1) Project [department_id#5605L, product_id0#5607L, product_id1#5610L, FLOOR((rand(5022767546056210753) * 20000.0)) AS product_id2#5614L]\n",
      "            :           :           :           :           :              +- *(1) Project [department_id#5605L, product_id0#5607L, FLOOR((rand(-6936063030933902556) * 20000.0)) AS product_id1#5610L]\n",
      "            :           :           :           :           :                 +- *(1) Project [(id#5603L % 15) AS department_id#5605L, FLOOR((rand(-6259683760911912666) * 20000.0)) AS product_id0#5607L]\n",
      "            :           :           :           :           :                    +- *(1) Range (0, 100000, step=1, splits=63)\n",
      "            :           :           :           :           +- *(2) Filter (isnotnull(department_id#0L) && isnotnull(product_id#1L))\n",
      "            :           :           :           :              +- Scan ExistingRDD[department_id#0L,product_id#1L,product_name#2]\n",
      "            :           :           :           +- *(5) Sort [department_id#5637L ASC NULLS FIRST, product_id#5638L ASC NULLS FIRST], false, 0\n",
      "            :           :           :              +- Exchange hashpartitioning(department_id#5637L, product_id#5638L, 200)\n",
      "            :           :           :                 +- *(4) Filter (isnotnull(department_id#5637L) && isnotnull(product_id#5638L))\n",
      "            :           :           :                    +- Scan ExistingRDD[department_id#5637L,product_id#5638L,product_name#5639]\n",
      "            :           :           +- *(9) Sort [department_id#5640L ASC NULLS FIRST, product_id#5641L ASC NULLS FIRST], false, 0\n",
      "            :           :              +- ReusedExchange [department_id#5640L, product_id#5641L, product_name#5642], Exchange hashpartitioning(department_id#5637L, product_id#5638L, 200)\n",
      "            :           +- *(13) Sort [department_id#5643L ASC NULLS FIRST, product_id#5644L ASC NULLS FIRST], false, 0\n",
      "            :              +- ReusedExchange [department_id#5643L, product_id#5644L, product_name#5645], Exchange hashpartitioning(department_id#5637L, product_id#5638L, 200)\n",
      "            +- *(17) Sort [department_id#5646L ASC NULLS FIRST, product_id#5647L ASC NULLS FIRST], false, 0\n",
      "               +- ReusedExchange [department_id#5646L, product_id#5647L, product_name#5648], Exchange hashpartitioning(department_id#5637L, product_id#5638L, 200)"
     ]
    }
   ],
   "source": [
    "sql_df.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 912.015869140625,
      "end_time": 1603741602898.077
     }
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(3) HashAggregate(keys=[product_name0#5680, product_name1#5689, product_name2#5699, product_name3#5710, product_name4#5722], functions=[count(1)])\n",
      "+- Exchange hashpartitioning(product_name0#5680, product_name1#5689, product_name2#5699, product_name3#5710, product_name4#5722, 200)\n",
      "   +- *(2) HashAggregate(keys=[product_name0#5680, product_name1#5689, product_name2#5699, product_name3#5710, product_name4#5722], functions=[partial_count(1)])\n",
      "      +- *(2) Project [pythonUDF0#5755 AS product_name0#5680, pythonUDF1#5756 AS product_name1#5689, pythonUDF2#5757 AS product_name2#5699, pythonUDF3#5758 AS product_name3#5710, pythonUDF4#5759 AS product_name4#5722]\n",
      "         +- BatchEvalPython [<lambda>(department_id#5605L, product_id0#5607L), <lambda>(department_id#5605L, product_id1#5610L), <lambda>(department_id#5605L, product_id2#5614L), <lambda>(department_id#5605L, product_id3#5619L), <lambda>(department_id#5605L, product_id4#5625L)], [department_id#5605L, product_id0#5607L, product_id1#5610L, product_id2#5614L, product_id3#5619L, product_id4#5625L, pythonUDF0#5755, pythonUDF1#5756, pythonUDF2#5757, pythonUDF3#5758, pythonUDF4#5759]\n",
      "            +- *(1) Project [department_id#5605L, product_id0#5607L, product_id1#5610L, product_id2#5614L, product_id3#5619L, FLOOR((rand(-1393926809299574983) * 20000.0)) AS product_id4#5625L]\n",
      "               +- *(1) Project [department_id#5605L, product_id0#5607L, product_id1#5610L, product_id2#5614L, FLOOR((rand(8479318887747898147) * 20000.0)) AS product_id3#5619L]\n",
      "                  +- *(1) Project [department_id#5605L, product_id0#5607L, product_id1#5610L, FLOOR((rand(5022767546056210753) * 20000.0)) AS product_id2#5614L]\n",
      "                     +- *(1) Project [department_id#5605L, product_id0#5607L, FLOOR((rand(-6936063030933902556) * 20000.0)) AS product_id1#5610L]\n",
      "                        +- *(1) Project [(id#5603L % 15) AS department_id#5605L, FLOOR((rand(-6259683760911912666) * 20000.0)) AS product_id0#5607L]\n",
      "                           +- *(1) Range (0, 100000, step=1, splits=63)"
     ]
    }
   ],
   "source": [
    "udf_df.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making sure it is the same data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the skeptical among us, here is a code to check that both dataframes, *sql_df* and *udf_df*, contain the same data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 33.265869140625,
      "end_time": 1603741695546.583
     }
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "columns_order = udf_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 44190.10400390625,
      "end_time": 1603741743880.607
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------+-------------+-------------+-------------+-----+\n",
      "|product_name0|product_name1|product_name2|product_name3|product_name4|count|\n",
      "+-------------+-------------+-------------+-------------+-------------+-----+\n",
      "+-------------+-------------+-------------+-------------+-------------+-----+"
     ]
    }
   ],
   "source": [
    "udf_df.subtract(sql_df.select(columns_order)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sql_df.select(columns_order).subtract(udf_df).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark3",
   "language": "",
   "name": "pyspark3kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark3",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
