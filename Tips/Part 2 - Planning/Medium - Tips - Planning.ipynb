{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import spark_partition_id, explode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tip #1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df1 = spark.range(2000000).selectExpr('(3*id) % 5 as column_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df2 = spark.range(2000000).selectExpr('id as column_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df1.join(df2, df1[\"column_1\"]==df2[\"column_2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+\n",
      "|column_1|column_2|\n",
      "+--------+--------+\n",
      "|       0|       0|\n",
      "|       0|       0|\n",
      "|       0|       0|\n",
      "+--------+--------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000000"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(5) SortMergeJoin [column_1#480L], [column_2#484L], Inner\n",
      ":- *(2) Sort [column_1#480L ASC NULLS FIRST], false, 0\n",
      ":  +- Exchange hashpartitioning(column_1#480L, 200), true, [id=#1820]\n",
      ":     +- *(1) Project [((3 * id#478L) % 5) AS column_1#480L]\n",
      ":        +- *(1) Filter isnotnull(((3 * id#478L) % 5))\n",
      ":           +- *(1) Range (0, 2000000, step=1, splits=4)\n",
      "+- *(4) Sort [column_2#484L ASC NULLS FIRST], false, 0\n",
      "   +- Exchange hashpartitioning(column_2#484L, 200), true, [id=#1826]\n",
      "      +- *(3) Project [id#482L AS column_2#484L]\n",
      "         +- *(3) Range (0, 2000000, step=1, splits=4)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tip #4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set('spark.sql.shuffle.partitions', 123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'123'"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.get('spark.sql.shuffle.partitions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df1.join(df2, df1[\"column_1\"]==df2[\"column_2\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The shuffling is now set to 123 !**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(5) SortMergeJoin [column_1#480L], [column_2#484L], Inner\n",
      ":- *(2) Sort [column_1#480L ASC NULLS FIRST], false, 0\n",
      ":  +- Exchange hashpartitioning(column_1#480L, 123), true, [id=#2448]\n",
      ":     +- *(1) Project [((3 * id#478L) % 5) AS column_1#480L]\n",
      ":        +- *(1) Filter isnotnull(((3 * id#478L) % 5))\n",
      ":           +- *(1) Range (0, 2000000, step=1, splits=4)\n",
      "+- *(4) Sort [column_2#484L ASC NULLS FIRST], false, 0\n",
      "   +- Exchange hashpartitioning(column_2#484L, 123), true, [id=#2454]\n",
      "      +- *(3) Project [id#482L AS column_2#484L]\n",
      "         +- *(3) Range (0, 2000000, step=1, splits=4)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tip #5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bins = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "array = f\"array({', '.join([str(i) for i in range(bins)])})\"\n",
    "df11 = df1.selectExpr('*', f'explode({array}) as salt')\n",
    "df11.createOrReplaceTempView('df11')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+\n",
      "|column_1|salt|\n",
      "+--------+----+\n",
      "|       0|   0|\n",
      "|       0|   1|\n",
      "|       0|   2|\n",
      "|       0|   3|\n",
      "|       0|   4|\n",
      "|       0|   5|\n",
      "|       0|   6|\n",
      "|       0|   7|\n",
      "|       0|   8|\n",
      "|       0|   9|\n",
      "|       0|  10|\n",
      "|       0|  11|\n",
      "|       0|  12|\n",
      "|       0|  13|\n",
      "|       0|  14|\n",
      "|       0|  15|\n",
      "|       0|  16|\n",
      "|       0|  17|\n",
      "|       0|  18|\n",
      "|       0|  19|\n",
      "+--------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df11.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "salt = f\"cast(random()*{bins} as int)\"\n",
    "df22 = df2.selectExpr(f\"concat_ws('_', column_2, {salt}) as salted_column_2\")\n",
    "df22.createOrReplaceTempView('df22')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "df0 = spark.sql(\"\"\"\n",
    "    select *\n",
    "    from df22 a \n",
    "    join df11 b\n",
    "    on a.salted_column_2 = concat_ws('_', b.column_1, b.salt)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(5) SortMergeJoin [salted_column_2#643], [concat_ws(_, cast(column_1#480L as string), cast(salt#636 as string))], Inner\n",
      ":- *(2) Sort [salted_column_2#643 ASC NULLS FIRST], false, 0\n",
      ":  +- Exchange hashpartitioning(salted_column_2#643, 200), true, [id=#2166]\n",
      ":     +- *(1) Project [concat_ws(_, cast(id#482L as string), cast(cast((random(2636194755330797495) * 100.0) as int) as string)) AS salted_column_2#643]\n",
      ":        +- *(1) Range (0, 2000000, step=1, splits=4)\n",
      "+- *(4) Sort [concat_ws(_, cast(column_1#480L as string), cast(salt#636 as string)) ASC NULLS FIRST], false, 0\n",
      "   +- Exchange hashpartitioning(concat_ws(_, cast(column_1#480L as string), cast(salt#636 as string)), 200), true, [id=#2173]\n",
      "      +- Generate explode([0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99]), [column_1#480L], false, [salt#636]\n",
      "         +- *(3) Project [((3 * id#478L) % 5) AS column_1#480L]\n",
      "            +- *(3) Range (0, 2000000, step=1, splits=4)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df0.explain()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
